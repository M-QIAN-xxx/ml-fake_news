{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection of Fake News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jackie Mason: Hollywood Would Love Trump if He...</td>\n",
       "      <td>Daniel Nussbaum</td>\n",
       "      <td>In these trying times, Jackie Mason is the Voi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Life: Life Of Luxury: Elton John’s 6 Favorite ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ever wonder how Britain’s most iconic pop pian...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Benoît Hamon Wins French Socialist Party’s Pre...</td>\n",
       "      <td>Alissa J. Rubin</td>\n",
       "      <td>PARIS  —   France chose an idealistic, traditi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title           author  \\\n",
       "0  FLYNN: Hillary Clinton, Big Woman on Campus - ...  Daniel J. Flynn   \n",
       "1  15 Civilians Killed In Single US Airstrike Hav...  Jessica Purkiss   \n",
       "2  Jackie Mason: Hollywood Would Love Trump if He...  Daniel Nussbaum   \n",
       "3  Life: Life Of Luxury: Elton John’s 6 Favorite ...              NaN   \n",
       "4  Benoît Hamon Wins French Socialist Party’s Pre...  Alissa J. Rubin   \n",
       "\n",
       "                                                text  label  \n",
       "0  Ever get the feeling your life circles the rou...      0  \n",
       "1  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "2  In these trying times, Jackie Mason is the Voi...      0  \n",
       "3  Ever wonder how Britain’s most iconic pop pian...      1  \n",
       "4  PARIS  —   France chose an idealistic, traditi...      0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "all_data = pd.read_csv(\"fake_news_train.csv\", sep = \",\", usecols=range(5))\n",
    "all_data['label'] = all_data['label'].astype(int)\n",
    "all_data = all_data.drop(columns = \"id\")\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check whether dataset contains any NaN values and drop rows that contains NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title     False\n",
       "author    False\n",
       "text      False\n",
       "label     False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.isna().any()\n",
    "all_data = all_data.dropna()\n",
    "all_data.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Split data to train and test (%20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train = all_data.sample(frac = 0.8) \n",
    "test = all_data.drop(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import stop_words\n",
    "stops = stop_words.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Delete stop words and make all words lowercase for bag-of-words. Also, using regular expressions discard numbers and punctuations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4454</th>\n",
       "      <td>trump spokesperson katrina pierson caught blat...</td>\n",
       "      <td>Andrew Bradford</td>\n",
       "      <td>trump spokesperson katrina pierson caught blat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12742</th>\n",
       "      <td>experts  isis root true islam defeated guns</td>\n",
       "      <td>dailouk</td>\n",
       "      <td>home   world   experts  isis root true islam d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>peak millennial  cities can t assume continued...</td>\n",
       "      <td>Conor Dougherty</td>\n",
       "      <td>past decade  american cities transformed young...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11098</th>\n",
       "      <td>singer kaya jones shares support trump  thanks...</td>\n",
       "      <td>Daniel Nussbaum</td>\n",
       "      <td>singer dj kaya jones took social media week do...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6988</th>\n",
       "      <td>nut job   new york times collaborates deep st...</td>\n",
       "      <td>Joel B. Pollak</td>\n",
       "      <td>new york times waited president donald trump s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title           author  \\\n",
       "4454   trump spokesperson katrina pierson caught blat...  Andrew Bradford   \n",
       "12742        experts  isis root true islam defeated guns          dailouk   \n",
       "1421   peak millennial  cities can t assume continued...  Conor Dougherty   \n",
       "11098  singer kaya jones shares support trump  thanks...  Daniel Nussbaum   \n",
       "6988    nut job   new york times collaborates deep st...   Joel B. Pollak   \n",
       "\n",
       "                                                    text  label  \n",
       "4454   trump spokesperson katrina pierson caught blat...      1  \n",
       "12742  home   world   experts  isis root true islam d...      1  \n",
       "1421   past decade  american cities transformed young...      0  \n",
       "11098  singer dj kaya jones took social media week do...      0  \n",
       "6988   new york times waited president donald trump s...      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['title'] = train['title'].apply(lambda x: ' '.join([re.sub(\"[^a-zA-Z]\",\" \",word.lower()) for word in x.split() if word.lower() not in stops]))\n",
    "train['text'] = train['text'].apply(lambda x: ' '.join([re.sub(\"[^a-zA-Z]\",\" \",word.lower()) for word in x.split() if word.lower() not in stops]))\n",
    "test['title'] = test['title'].apply(lambda x: ' '.join([re.sub(\"[^a-zA-Z]\",\" \",word.lower()) for word in x.split() if word.lower() not in stops]))\n",
    "test['text'] = test['text'].apply(lambda x: ' '.join([re.sub(\"[^a-zA-Z]\",\" \",word.lower()) for word in x.split() if word.lower() not in stops]))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "counts = train.groupby(by = \"label\", as_index=False).count()\n",
    "number_of_real_news = counts.at[0, \"title\"]\n",
    "number_of_fake_news = counts.at[1, \"title\"]\n",
    "number_of_total_news = number_of_fake_news + number_of_real_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Separate fake and real news to different datasets\n",
    "train_fake = train.drop(train[train.label == 0].index)\n",
    "train_real = train.drop(train[train.label == 1].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Function to return n-gram version of the string s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_ngram(string, n):\n",
    "    string = string.lower()\n",
    "    tokens = [token for token in string.split(\" \") if token != \"\"]  \n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "n-gram Naive bayes implementation with Laplace smoothing and logarithmic calculation to avoid underflow while multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def naive_bayes(text, vocabulary, type_matrix_sum, type_total_news, type_total_words, n):\n",
    "    text_split = make_ngram(text, n)\n",
    "    type_prob = type_total_news / number_of_total_news      # Fake or real news probability across all news\n",
    "    denominator = type_total_words + total_unique_words     # Denominator for laplace smoothing\n",
    "    multiplied_log_probability = math.log(type_prob)\n",
    "    for word in text_split:                                 # Find each word's probability\n",
    "        try:\n",
    "            word_index = vocabulary[word]\n",
    "            number_of_appearances = type_matrix_sum[0,word_index]\n",
    "        except KeyError:\n",
    "            number_of_appearances = 0\n",
    "        number_of_appearances += 1  # Laplace smoothing \n",
    "        prob = number_of_appearances / denominator\n",
    "        prob = math.log(prob)       # avoid underflow\n",
    "        multiplied_log_probability += prob\n",
    "\n",
    "    return multiplied_log_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Prediction from Titles\n",
    "\n",
    "titles_cv = CountVectorizer(strip_accents=\"ascii\", ngram_range=(1, 1))\n",
    "titles_real_cv = CountVectorizer(strip_accents=\"ascii\", ngram_range=(1, 1))\n",
    "titles_fake_cv = CountVectorizer(strip_accents=\"ascii\", ngram_range=(1, 1))\n",
    "\n",
    "title_fitted = titles_cv.fit(train[\"title\"])\n",
    "total_unique_words = len(title_fitted.vocabulary_)\n",
    "\n",
    "fake_news_words_matrix = titles_fake_cv.fit_transform(train_fake[\"title\"])\n",
    "total_fake_words = fake_news_words_matrix.count_nonzero()\n",
    "real_news_words_matrix = titles_real_cv.fit_transform(train_real[\"title\"])\n",
    "total_real_words = real_news_words_matrix.count_nonzero()\n",
    "\n",
    "sum_words_fake = fake_news_words_matrix.sum(axis = 0)\n",
    "fake_words_freq = [(word, sum_words_fake[0, idx]) for word, idx in titles_fake_cv.vocabulary_.items()]\n",
    "fake_words_freq = sorted(fake_words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "sum_words_real = real_news_words_matrix.sum(axis = 0)\n",
    "real_words_freq = [(word, sum_words_real[0, idx]) for word, idx in titles_real_cv.vocabulary_.items()]\n",
    "real_words_freq = sorted(real_words_freq, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Find title prediction accuracy\n",
    "\n",
    "total_test = len(test[\"title\"])\n",
    "correct = 0\n",
    "\n",
    "for i in range(total_test):\n",
    "    correct_label = test[\"label\"].iloc[i]\n",
    "    fake_prob = naive_bayes(test[\"title\"].iloc[i], titles_fake_cv.vocabulary_, sum_words_fake, number_of_fake_news, total_fake_words, 1) \n",
    "    real_prob = naive_bayes(test[\"title\"].iloc[i], titles_real_cv.vocabulary_, sum_words_real, number_of_real_news, total_real_words, 1)\n",
    "    if real_prob > fake_prob:\n",
    "        result_label = 0\n",
    "    else:\n",
    "        result_label = 1\n",
    "    if result_label == correct_label:\n",
    "        correct += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION FROM TITLE ACCURACY : 90.07529089664614\n"
     ]
    }
   ],
   "source": [
    "print(\"PREDICTION FROM TITLE ACCURACY : \" + str(100 * (correct / total_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "  \n",
    "**Most 10 Frequent Words in Fake News Headlines:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('trump', 793), ('hillary', 577), ('clinton', 480), ('election', 242), ('new', 238), ('video', 211), ('comment', 211), ('war', 192), ('fbi', 178), ('world', 158)]\n"
     ]
    }
   ],
   "source": [
    "print(fake_words_freq[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most 10 Frequent Words in Real News Headlines:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('new', 4286), ('york', 4064), ('times', 4029), ('breitbart', 1499), ('trump', 1474), ('donald', 413), ('obama', 197), ('clinton', 192), ('says', 183), ('briefing', 156)]\n"
     ]
    }
   ],
   "source": [
    "print(real_words_freq[:10])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It is clear to see that 3 most common words which appear in reliable news titles are **\"new\"**(4272 times), **\"york\"**(4045), \n",
    "**\"times\"**(4012). Which make sense when we think about New York Times is a very reputable newspaper, so most of the news which include these words \n",
    "are reliable news. These 3 words together appeared **more than 4000 times** in real news headlines. However, they almost never appeared together in \n",
    "a fake news headline.\n",
    "\n",
    "For fake news headlines, majority of the words are about politics and elections. \"trump\", \"hillary\" and \"clinton\" words are ahead\n",
    "of other words by far. However, \"trump\" word appears in real news headlines two times more than in fake news headlines. So, it\n",
    "is not possible to make sense from \"trump\" word. On the other hand, **\"hillary\"**(587 times), **\"clinton\"**(486) and **\"election\"**(259) \n",
    "words are appear in fake news headlines way more than real news headlines.   \n",
    "\n",
    "When we make predictions only on news titles, accuracy score is almost %90 which is really well. This can be a result of\n",
    "above inferences. Titles are very short sentences and there are some very weighted words which specified above.\n",
    "With this words, it is easier to make predictions about news labels. **So it is very feasible to predict whether \n",
    "a headline is from a real or fake news from words that appear in the headline.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Prediction from texts 1-Gram\n",
    "\n",
    "texts_cv = CountVectorizer(strip_accents=\"ascii\", ngram_range=(0, 1))\n",
    "texts_real_cv = CountVectorizer(strip_accents=\"ascii\", ngram_range=(0, 1))\n",
    "texts_fake_cv = CountVectorizer(strip_accents=\"ascii\", ngram_range=(0, 1))\n",
    "\n",
    "text_fitted = texts_cv.fit(train[\"text\"])\n",
    "total_unique_words = len(text_fitted.vocabulary_)\n",
    "\n",
    "fake_news_words_matrix_text = texts_fake_cv.fit_transform(train_fake[\"text\"])\n",
    "total_fake_words_text = fake_news_words_matrix_text.count_nonzero()\n",
    "\n",
    "real_news_words_matrix_text = texts_real_cv.fit_transform(train_real[\"text\"])\n",
    "total_real_words_text = real_news_words_matrix_text.count_nonzero()\n",
    "\n",
    "fake_news_words_matrix_text_sum = fake_news_words_matrix_text.sum(axis = 0)\n",
    "real_news_words_matrix_text_sum = real_news_words_matrix_text.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-GRAM TEXT ACCURACY : 90.99931553730322\n"
     ]
    }
   ],
   "source": [
    "# Find 1-Gram text prediction accuracy\n",
    "correct = 0\n",
    "for i in range(total_test):\n",
    "    correct_label = test[\"label\"].iloc[i]\n",
    "    fake_prob = naive_bayes(test[\"text\"].iloc[i], texts_fake_cv.vocabulary_, fake_news_words_matrix_text_sum, number_of_fake_news, total_fake_words_text, 1) \n",
    "    real_prob = naive_bayes(test[\"text\"].iloc[i], texts_real_cv.vocabulary_, real_news_words_matrix_text_sum, number_of_real_news, total_real_words_text, 1)\n",
    "    if real_prob > fake_prob:\n",
    "        result_label = 0\n",
    "    else:\n",
    "        result_label = 1\n",
    "    if result_label == correct_label:\n",
    "        correct += 1\n",
    "     \n",
    "print(\"1-GRAM TEXT ACCURACY : \" + str(100 * (correct / total_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Prediction from texts 2-Gram\n",
    "\n",
    "texts_cv = CountVectorizer(strip_accents=\"ascii\", ngram_range=(2, 2))\n",
    "texts_real_cv = CountVectorizer(strip_accents=\"ascii\", ngram_range=(2, 2))\n",
    "texts_fake_cv = CountVectorizer(strip_accents=\"ascii\", ngram_range=(2, 2))\n",
    "\n",
    "text_fitted = texts_cv.fit(train[\"text\"])\n",
    "total_unique_words = len(text_fitted.vocabulary_)\n",
    "fake_news_words_matrix_text = texts_fake_cv.fit_transform(train_fake[\"text\"])\n",
    "total_fake_words_text = fake_news_words_matrix_text.count_nonzero()\n",
    "\n",
    "real_news_words_matrix_text = texts_real_cv.fit_transform(train_real[\"text\"])\n",
    "total_real_words_text = real_news_words_matrix_text.count_nonzero()\n",
    "\n",
    "fake_news_words_matrix_text_sum = fake_news_words_matrix_text.sum(axis = 0)\n",
    "real_news_words_matrix_text_sum = real_news_words_matrix_text.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-GRAM TEXT ACCURACY : 90.86242299794661\n"
     ]
    }
   ],
   "source": [
    "# Find 2-gram text prediction accuracy\n",
    "\n",
    "correct = 0\n",
    "for i in range(total_test):\n",
    "    correct_label = test[\"label\"].iloc[i]\n",
    "    fake_prob = naive_bayes(test[\"text\"].iloc[i], texts_fake_cv.vocabulary_, fake_news_words_matrix_text_sum, number_of_fake_news, total_fake_words_text, 2) \n",
    "    real_prob = naive_bayes(test[\"text\"].iloc[i], texts_real_cv.vocabulary_, real_news_words_matrix_text_sum, number_of_real_news, total_real_words_text, 2)\n",
    "    if real_prob > fake_prob:\n",
    "        result_label = 0\n",
    "    else:\n",
    "        result_label = 1\n",
    "    if result_label == correct_label:\n",
    "        correct += 1\n",
    "     \n",
    "print(\"2-GRAM TEXT ACCURACY : \" + str(100 * (correct / total_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As expected, when we use bi-gram instead of uni-gram, accuracy score is little bit higher. When we use bi-gram, our word matrix is trying to understand word groups\n",
    "such as \"new york\", \"donald trump\" etc. . This way it is more likely to predict the correct class for articles. \n",
    "\n",
    "Also, it is possible to make accuracy score even higher with use of 3-gram or 4-gram. For example, when using 3-gram, CountVectorizer will make sense from\n",
    "\"new york times\" or \"national football league\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "texts_tf_cv = TfidfVectorizer(strip_accents=\"ascii\", ngram_range=(1, 1), max_features=100000)\n",
    "texts_real_tf_cv = TfidfVectorizer(strip_accents=\"ascii\", ngram_range=(1, 1), max_features=100000)\n",
    "texts_fake_tf_cv = TfidfVectorizer(strip_accents=\"ascii\", ngram_range=(1, 1), max_features=100000)\n",
    "\n",
    "text_fitted_tf = texts_tf_cv.fit(train[\"text\"])\n",
    "total_unique_words_tf = len(text_fitted_tf.vocabulary_)\n",
    "\n",
    "fake_news_words_matrix_text_tf = texts_fake_tf_cv.fit_transform(train_fake[\"text\"])\n",
    "total_fake_words_text_tf = fake_news_words_matrix_text_tf.count_nonzero()\n",
    "real_news_words_matrix_text_tf = texts_real_tf_cv.fit_transform(train_real[\"text\"])\n",
    "total_real_words_text_tf = real_news_words_matrix_text_tf.count_nonzero()\n",
    "\n",
    "fake_news_words_matrix_text_sum_tf = fake_news_words_matrix_text_tf.sum(axis = 0)\n",
    "real_news_words_matrix_text_sum_tf = real_news_words_matrix_text_tf.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT ACCURACY TF-IDF: 80.66392881587954\n"
     ]
    }
   ],
   "source": [
    "# Find TF-IDF text prediction accuracy\n",
    "\n",
    "correct = 0\n",
    "for i in range(total_test):\n",
    "    correct_label = test[\"label\"].iloc[i]\n",
    "    fake_prob = naive_bayes(test[\"text\"].iloc[i], texts_fake_tf_cv.vocabulary_, fake_news_words_matrix_text_sum_tf, number_of_fake_news, total_fake_words_text_tf, 1) \n",
    "    real_prob = naive_bayes(test[\"text\"].iloc[i], texts_real_tf_cv.vocabulary_, real_news_words_matrix_text_sum_tf, number_of_real_news, total_real_words_text_tf, 1)\n",
    "    if real_prob > fake_prob:\n",
    "        result_label = 0\n",
    "    else:\n",
    "        result_label = 1\n",
    "    if result_label == correct_label:\n",
    "        correct += 1\n",
    "     \n",
    "print(\"TEXT ACCURACY TF-IDF: \" + str(100 * (correct / total_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "When narrowing our features with **max_features** parameter, accuracy of TF-IDF is a little bit higher than before. This way, only counting\n",
    "most weighted 100.000 words in our texts. That confirms **classification results can be improved by selecting a subset of extremely effective \n",
    "words for the dictionary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fake_weight = fake_news_words_matrix_text_tf.sum(axis = 0)\n",
    "fake_words_weights = [(word, fake_weight[0, idx]) for word, idx in texts_fake_tf_cv.vocabulary_.items()]\n",
    "fake_words_weights = sorted(fake_words_weights, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10 words whose presence most strongly predicts that the news is fake:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('trump', 194.9549280508746), ('clinton', 163.9016865652972), ('hillary', 126.00582193245671), ('people', 97.4289833401401), ('it', 90.9163909334322), ('election', 85.99562267619844), ('said', 83.22597802510128), ('new', 70.76061452954936), ('fbi', 70.5707168328888), ('obama', 68.76582479942536)]\n"
     ]
    }
   ],
   "source": [
    "print(fake_words_weights[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10 words whose absence most strongly predicts that the news is fake:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('shean', 0.004446212888346782), ('beisan', 0.004446212888346782), ('mortared', 0.004446212888346782), ('unbending', 0.004446212888346782), ('palmah', 0.004446212888346782), ('omelet', 0.004446212888346782), ('defensively', 0.004446212888346782), ('inhabitancy', 0.004446212888346782), ('fialkoff', 0.004446212888346782), ('jeremyrhammond', 0.004446212888346782)]\n"
     ]
    }
   ],
   "source": [
    "print(fake_words_weights[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "real_weight = real_news_words_matrix_text_tf.sum(axis = 0)\n",
    "real_words_weights = [(word, real_weight[0, idx]) for word, idx in texts_fake_tf_cv.vocabulary_.items()]\n",
    "real_words_weights = sorted(real_words_weights, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10 words whose presence most strongly predicts that the news is real:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('narcisstic', 375.73381148089294), ('santelli', 308.7544867750158), ('unheard', 297.5939964871302), ('prohibited', 135.27438595627038), ('inviting', 124.24292018909804), ('philth', 119.9454329006608), ('norman', 114.53135995459839), ('narrates', 100.2017257583236), ('collins', 92.86192143269939), ('liita', 90.79738769803623)]\n"
     ]
    }
   ],
   "source": [
    "print(real_words_weights[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10 words whose absence most strongly predicts that the news is real:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('captivity', 0.004952348433626786), ('fff', 0.004952348433626786), ('derma', 0.004952348433626786), ('tester', 0.004952348433626786), ('amerikkkan', 0.004952348433626786), ('escuchando', 0.004952348433626786), ('subterr', 0.004952348433626786), ('resealed', 0.004952348433626786), ('veganer', 0.004952348433626786), ('machu', 0.004952348433626786)]\n"
     ]
    }
   ],
   "source": [
    "print(real_words_weights[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing effect of the stopwords\n",
    "Stopwords are most common words in languages. Such as \"the\",\"and\",\"or\",\"with\" etc. In CountVectorizer method, if stop words are dont get deleted \n",
    "they will be most recent words, so it would be impossible to make any valuable predictions. However, in TF-IDF, because of all words have a \n",
    "weight, stopwords will have least weights. In that situation, stopwords won't be as important as in CountVectorizer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}